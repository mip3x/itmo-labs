# -*- coding: utf-8 -*-
"""lab6.ipynb

Automatically generated by Colab.

# Лабораторная 6. Логистическая регрессия
"""

import pandas as pd

data = pd.read_csv("titanic.csv")
data

"""Удалим признак `Name`, так как он является идентификатором и не несёт полезной информации"""

data = data.drop(columns=["Name"])

"""`Sex` - категориальный признак. Его можно закодировать:"""

data["Sex"] = data["Sex"].map({
    "male": 0,
    "female": 1
})
data

"""## Обработка данных"""

data.describe()

data.info()

data.isnull().sum()

"""Пропусков нет

## Визуализация
"""

data.hist(bins=30, figsize=(15, 10))

"""`Survived` - целевой признак"""

X = data.drop("Survived", axis=1)
y = data["Survived"]
X

y

"""## Разделение данных на обучающий и тестовый наборы"""

import numpy as np

np.random.seed(42)
n = len(X)
indices = np.random.permutation(n)

train_size = int(0.8 * n)

train_idx = indices[:train_size]
test_idx = indices[train_size:]

X_train = X.iloc[train_idx].copy()
X_test = X.iloc[test_idx].copy()

y_train = y.iloc[train_idx].copy()
y_test = y.iloc[test_idx].copy()

"""## Z-score нормализация"""

X_train_norm = X_train.copy()
X_test_norm = X_test.copy()

for col in X_train_norm.columns:
    mean = X_train_norm[col].mean()
    std = X_train_norm[col].std()

    if std == 0 or np.isnan(std):
        X_train_norm[col] = 0.0
        X_test_norm[col]  = 0.0
    else:
        X_train_norm[col] = (X_train_norm[col] - mean) / std
        X_test_norm[col] = (X_test_norm[col]  - mean) / std

X_train_norm.hist(bins=100, figsize=(15, 15))

"""## Реализация логистической регрессии"""

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def log_loss(y_true, y_pred):
    eps = 1e-15
    y_pred = np.clip(y_pred, eps, 1 - eps)
    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

"""### 1-ый метод оптимизации: градиентный спуск"""

def fit_logreg_gd(X, y, lr=0.1, n_iter=2000):
    n_samples, n_features = X.shape
    w = np.zeros(n_features) # веса признаков
    losses = []

    # итерация - шаг градиентного спуска
    for i in range(n_iter):
        # линейная комбинация
        z = X @ w

        # вероятности
        p = sigmoid(z)

        # loss
        losses.append(log_loss(y, p))

        # градиент
        grad = (X.T @ (p - y)) / n_samples

        # обновление весов
        w -= lr * grad

    return w, losses

"""Обновление весов и смещения в направлении, противоположном градиенту, с использованием заданной скорости обучения, является ключевым шагом в градиентном спуске. Этот подход направлен на минимизацию функции потерь и обеспечивает сходимость алгоритма обучения. обновление весов и смещения в направлении, противоположном градиенту, помогает двигаться в сторону минимума функции потерь.

### 2-ой метод оптимизации: Ньютона
"""

def fit_logreg_newton(X, y, n_iter=30):
    n_samples, n_features = X.shape
    w = np.zeros(n_features)
    losses = []

    for _ in range(n_iter):
        z = X @ w

        p = sigmoid(z)

        losses.append(log_loss(y, p))

        grad = (X.T @ (p - y)) / n_samples

        # матрица Гессе
        r = p * (1 - p) # (n_samples,)
        H = (X.T * r) @ X / n_samples # (n_features, n_features)

        # шаг Ньютона
        step = np.linalg.solve(H + 1e-8*np.eye(n_features), grad)
        w -= step

    return w, losses

"""Метод Ньютона обычно сходится быстрее, чем градиентный спуск, но может быть более вычислительно затратным из-за вычисления обратной матрицы Гессе. Он также может быть более чувствителен к начальной точке, чем градиентный спуск."""

def predict_proba(X, w):
    return sigmoid(X @ w)

def predict(X, w, threshold=0.5):
    return (predict_proba(X, w) >= threshold).astype(int)

Xtr = X_train_norm.to_numpy(dtype=float)
Xte = X_test_norm.to_numpy(dtype=float)

ytr = y_train.to_numpy(dtype=float)
yte = y_test.to_numpy(dtype=float)

"""## Реализация метрик"""

def accuracy(y_true, y_pred):
    TP = np.sum((y_true == 1) & (y_pred == 1))
    TN = np.sum((y_true == 0) & (y_pred == 0))
    FP = np.sum((y_true == 0) & (y_pred == 1))
    FN = np.sum((y_true == 1) & (y_pred == 0))
    return (TP + TN) / (TP + TN + FP + FN)

def precision(y_true, y_pred):
    tp = np.sum((y_true == 1) & (y_pred == 1))
    fp = np.sum((y_true == 0) & (y_pred == 1))
    return tp / (tp + fp + 1e-15)

def recall(y_true, y_pred):
    tp = np.sum((y_true == 1) & (y_pred == 1))
    fn = np.sum((y_true == 1) & (y_pred == 0))
    return tp / (tp + fn + 1e-15)

def f1_score(y_true, y_pred):
    p = precision(y_true, y_pred)
    r = recall(y_true, y_pred)
    return 2*p*r / (p + r + 1e-15)

"""## Исследование гиперпараметров"""

learning_rates = [0.001, 0.01, 0.1, 0.3]
iters_gd = [200, 500, 1000, 2000]
iters_newton = [5, 10, 20, 30]

"""Градиентный спуск"""

results = []

for lr in learning_rates:
    for n_iter in iters_gd:
        w, _ = fit_logreg_gd(Xtr, ytr, lr=lr, n_iter=n_iter)
        y_pred = predict(Xte, w)

        results.append({
            "method": "gradient_descent",
            "learning_rate": lr,
            "n_iter": n_iter,
            "accuracy": accuracy(yte, y_pred),
            "precision": precision(yte, y_pred),
            "recall": recall(yte, y_pred),
            "f1": f1_score(yte, y_pred)
        })

"""Метод Ньютона"""

for n_iter in iters_newton:
    w, _ = fit_logreg_newton(Xtr, ytr, n_iter=n_iter)
    y_pred = predict(Xte, w)

    results.append({
        "method": "newton",
        "learning_rate": None,
        "n_iter": n_iter,
        "accuracy": accuracy(yte, y_pred),
        "precision": precision(yte, y_pred),
        "recall": recall(yte, y_pred),
        "f1": f1_score(yte, y_pred)
    })

"""Таблица результатов"""

results_df = pd.DataFrame(results)
results_df.sort_values(by="f1", ascending=False)

"""Выбор лучшей модели"""

best = results_df.sort_values(by="f1", ascending=False).iloc[0]
best

"""По результатам экспериментов установлено, что для градиентного спуска наилучшее качество классификации достигается при значениях коэффициента обучения `learning rate` в диапазоне 0.1–0.3 и достаточно большом числе итераций обучения (1000–2000). При меньших значениях `learning rate` (например, 0.001) модель сходится значительно медленнее и демонстрирует более низкие значения метрик при фиксированном числе итераций, что указывает на недообучение.

Увеличение числа итераций обучения приводит к улучшению качества модели лишь до определённого момента. После достижения минимума функции потерь дальнейшее увеличение числа итераций не оказывает заметного влияния на значения метрик `accuracy`, `precision`, `recall` и `F1-score`, что свидетельствует о сходимости алгоритма.

Метод оптимизации Ньютона демонстрирует сопоставимое качество классификации с градиентным спуском, достигая максимальных значений метрик уже при 5–10 итерациях обучения. Это подтверждает более быструю сходимость метода Ньютона по сравнению с градиентным спуском. Однако следует учитывать, что каждая итерация метода Ньютона вычислительно более затратна, так как требует вычисления и обращения матрицы Гессе.

Таким образом, для данного набора данных обе стратегии оптимизации приводят к схожему качеству классификации, однако метод Ньютона обеспечивает более быструю сходимость, в то время как градиентный спуск является более простым и универсальным методом, требующим подбора коэффициента обучения и большего числа итераций.
"""
