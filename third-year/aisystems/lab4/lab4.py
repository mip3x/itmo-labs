# -*- coding: utf-8 -*-
"""lab4.ipynb

Automatically generated by Colab.

# Лабораторная работа №4

# Описание датасета из источника
1) Alcohol
2) Malic acid
3) Ash
4) Alcalinity of ash
5) Magnesium
6) Total phenols
7) Flavanoids
8) Nonflavanoid phenols
9) Proanthocyanins
10)Color intensity
11)Hue
12)OD280/OD315 of diluted wines
13)Proline
"""

import pandas as pd

data = pd.read_csv('/content/WineDataset.csv')
data

"""## Получение и отображение статистики
Получите и визуализируйте (графически) статистику по датасету (включая количество, среднее значение, стандартное отклонение, минимум, максимум и различные квантили), постройте 3d-визуализацию признаков
"""

statistics = data.describe()
statistics

"""Визуализация"""

data.hist(bins = 100, figsize = (15, 15))

"""## Предварительная обработка данных
Проведите предварительную обработку данных, включая обработку отсутствующих значений, кодирование категориальных признаков и масштабирование

Выясним, какие столбцы содержат пропуски:
"""

data.isna().sum()

"""Никакие не содержат. Следовательно, никакая обработка отсутствующих значений не требуется. Требуется ли кодирование категориальных признаков? Также не требуется, так как таких признаков нет, есть только числовые. **Wine** является не категориальным признаком, а целевой переменной.

## Нормировка (масштабирование) данных
Используем стандартизацию (Z-Score Normalization):
"""

X = data.drop("Wine", axis=1)
y = data["Wine"].to_numpy()

data_norm = X.copy()
for col in data_norm.columns:
    mean = data_norm[col].mean()
    std = data_norm[col].std()
    data_norm[col] = (data_norm[col] - mean) / std
data_norm.hist(bins = 100, figsize = (15, 15))

"""## 3d-визуализация

"""

from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import numpy as np

import random

random.seed(42)

r_columns = list(data_norm.columns)
random.shuffle(r_columns)
r_columns = r_columns[:3]

print("Выбранные признаки:", r_columns)

colors = {1: "red", 2: "green", 3: "blue"}

df = data_norm[r_columns].copy()
df["Wine"] = data["Wine"].values

print(df.head())
print("Колонки df:", df.columns)

colors = {1: "red", 2: "green", 3: "blue"}

fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection="3d")

for cls in sorted(df["Wine"].unique()):
    values = df[df["Wine"] == cls]
    ax.scatter(
        values[r_columns[0]],
        values[r_columns[1]],
        values[r_columns[2]],
        color=colors.get(cls, "black"),
        label=f"Wine {cls}",
        s=40,
        alpha=0.8
    )

ax.set_xlabel(r_columns[0] + " (normalized)")
ax.set_ylabel(r_columns[1] + " (normalized)")
ax.set_zlabel(r_columns[2] + " (normalized)")
ax.set_title("3D-визуализация нормализованных признаков")
ax.legend()

plt.show()

"""## Реализация метода k-NN

Реализуйте метод k-ближайших соседей без использования сторонних библиотек, кроме NumPy и Pandas

"""

def euclidean_distance(x1, x2):
    return np.sqrt(np.sum((x1 - x2) ** 2))

# X:               признаки
# y:               метки классов
# query_point:     точка для которой выполняется классификация
# k:               количество ближайших соседей, которые будут учтены
def k_nearest_neighbors(X, y, query_point, k):
    # Вычисляем расстояние между точками в многомерном пространстве
    distances = [euclidean_distance(query_point, x) for x in X]

    # Получаем точки с наименьшими расстояниями
    k_indices = np.argsort(distances)[:k]
    k_nearest_labels = [y[i] for i in k_indices]

    # Берем самый часто встречающийся [класс]
    most_common = np.bincount(k_nearest_labels).argmax()

    return most_common

"""## Построение моделей k-NN

Постройте две модели k-NN с различными наборами признаков:

1. Признаки случайно отбираются .
2. Фиксированный набор признаков, который выбирается заранее.

"""

# разделение на train/test (80/20)
def train_test_split_indices(n_samples, test_size=0.2, random_state=42):
    np.random.seed(random_state)

    indices = np.arange(n_samples)
    np.random.shuffle(indices)

    test_count = int(n_samples * test_size)

    test_idx = indices[:test_count]
    train_idx = indices[test_count:]

    return train_idx, test_idx

y = data["Wine"]
train_idx, test_idx = train_test_split_indices(len(data_norm), test_size=0.2, random_state=42)

X_train = data_norm.iloc[train_idx]
y_train = y[train_idx]

X_test = data_norm.iloc[test_idx]
y_test = y[test_idx]


def confusion_matrix(y_true, y_pred):
    labels = sorted(set(y_true))
    n = len(labels)

    cm = np.zeros((n, n), dtype=int)

    for t, p in zip(y_true, y_pred):
        i = labels.index(t) # строка - настоящий класс
        j = labels.index(p) # столбец - предсказанный класс
        cm[i][j] += 1

    return cm, labels


def print_confusion_matrix(cm, labels):
    df = pd.DataFrame(
        cm,
        index=[f"true_{l}" for l in labels],
        columns=[f"pred_{l}" for l in labels]
    )
    print(df)

"""Модель 1: случайный набор признаков"""

def build_model_random_features(X_train_full, X_test_full, n_features=4, random_state=42):
    np.random.seed(random_state)

    all_features = list(X_train_full.columns)
    chosen = np.random.choice(all_features, size=n_features, replace=False)
    chosen = list(chosen)

    X_train_sub = X_train_full[chosen].values
    X_test_sub = X_test_full[chosen].values

    return chosen, X_train_sub, X_test_sub

model1_features, X_train_m1, X_test_m1 = build_model_random_features(
    X_train, X_test, n_features=4, random_state=42
)
print("Модель 1, случайные признаки:", model1_features)

"""Модель 2: фиксированный набор признаков"""

def build_model_fixed_features(X_train_full, X_test_full, feature_list):
    X_train_sub = X_train_full[feature_list].values
    X_test_sub = X_test_full[feature_list].values

    return X_train_sub, X_test_sub

model2_features = ["Flavanoids", "OD280/OD315 of diluted wines", "Proline"]

X_train_m2, X_test_m2 = build_model_fixed_features(
    X_train, X_test, model2_features
)

print("Модель 2, фиксированные признаки:", model2_features)

"""

Оценка моделей на разных k и построение матриц ошибок"""

def knn_predict_batch(X_train, y_train, X_test, k):
    preds = []

    for q in X_test:
        preds.append(k_nearest_neighbors(X_train, y_train, q, k))

    return np.array(preds)

def evaluate_knn_model(X_train, y_train, X_test, y_test, k_list, model_name=""):
    for k in k_list:
        y_pred = knn_predict_batch(X_train, y_train, X_test, k)

        accuracy = (y_pred == y_test).mean()

        # матрица ошибок
        cm, labels = confusion_matrix(y_test, y_pred)

        print(f"=== {model_name} | k = {k} ===")
        print(f"Accuracy: {accuracy:.4f}")
        print_confusion_matrix(cm, labels)

k_values = [3, 5, 10]

# Модель 1 (случайные признаки)
evaluate_knn_model(
    X_train_m1, y_train.values,
    X_test_m1, y_test.values,
    k_list=k_values,
    model_name=f"Модель 1 (случайные признаки: {model1_features})"
)

# Модель 2 (фиксированные признаки)
evaluate_knn_model(
    X_train_m2, y_train.values,
    X_test_m2, y_test.values,
    k_list=k_values,
    model_name=f"Модель 2 (фиксированные признаки: {model2_features})"
)
